from src.summarizer import summarize_text
from src.utils import save_to_csv, calculate_rouge_l

# Function to compare summaries generated by different language models and evaluate using ROUGE-L
def compare_llm_summaries(relevant_text, topic, length='medium', reference_summary=None):
    # Call the summarizer to generate summaries from multiple models
    comparison = summarize_text(relevant_text, topic, length)

    if not comparison:
        print("No summaries were generated.")
        return

    data_for_csv = []
    
    # Iterate through the models and their generated summaries
    for model_name, summary in comparison.items():
        if summary:
            print(f"Model: {model_name}\nSummary:\n{summary}\n")
            avg_summary_length = len(summary.split())

            # If a reference summary is provided, calculate ROUGE-L score
            if reference_summary:
                rouge_score = calculate_rouge_l(reference_summary, summary)
                print(f"ROUGE-L F1 Score for {model_name}: {rouge_score}")
            else:
                rouge_score = None  # Skip if no reference summary provided

            # Collect data for saving to CSV
            data_for_csv.append([model_name, relevant_text, summary, avg_summary_length, rouge_score])
        else:
            print(f"Model: {model_name} failed to generate a summary.\n")
    
    # Save the comparison results to a CSV file
    save_to_csv(data_for_csv, filename="summary_comparison.csv")
    
    return comparison
